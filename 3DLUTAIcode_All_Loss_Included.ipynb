{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "243f7097",
   "metadata": {},
   "source": [
    "Adaptive 3D LUT - All Loss included\n",
    "==============\n",
    "\n",
    "## 1. Abstract\n",
    "현재 내가 맡은 파트는 Image-Adaptive-3DLUT 프로그램을 적절하게 수정하여, 현 최신 버전의 Cuda 및 Pytorch 등과 호환되도록 만들고, 저조도 image를 고조도 image로 바꾸는 작업을 AI model을 통해 수행하는 것이 목표다.\n",
    "또한, 이를 Jupyter Notebook 파일인 .ipynb로 이를 작성하여, 내가 작성한 코드와 준비 과정 기록의 가독성을 높이는 것을 주 목적으로 이 파일을 작성한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b33afd",
   "metadata": {},
   "source": [
    "### 1.1 Requirements (For Manual)\n",
    "내가 현재 사용중인 컴퓨터 사양 및, 필요한 프로그램과 패키지들의 설치, Python 가상환경 생성 등에 대해 다룬다.\n",
    "\n",
    "해당 코드를 가상 환경에서 실행하면, 내가 사용중인 모듈들과 정확히 동일한 버전의 Python 패키지들을 다운로드 받을 수 있다.\n",
    "\n",
    "이전에, **반드시 Python installation 과정에서 Add PATH 항목을 체크하고 넘어갈 것.** (설치 과정 사진 첨부 必)\n",
    "\n",
    "아래 코드들은, Python 3.10.4, Windows 11 64bit 환경의 Windows Powershell에서 진행된다고 가정한다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "060787d5",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (393043472.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[35], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    cd C:\\Users\\Username # Username 필드에 자신의 Username을 입력한다.\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "cd C:\\Users\\Username # Username 필드에 자신의 Username을 입력한다. \n",
    "python -m venv title # Title 필드에 자신의 가상항목 이름을 입력한다.\n",
    ".\\venv\\Scripts\\activate # 가상환경 활성화 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34c418a",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "### CUDA 12.1, 12.6 Capable\n",
    "numpy == 1.26.4\n",
    "pytorch --index-url https://download.pytorch.org/whl/cu126\n",
    "torchvision --index-url https://download.pytorch.org/whl/cu126\n",
    "Pillow == 9.5.0\n",
    "opencv-python == 4.8.0.76\n",
    "scipy == 1.15.3\n",
    "ninja == 1.11.1.4\n",
    "imageio == 2.37.0\n",
    "kornia == 0.8.1\n",
    "wheel == 0.45.1\n",
    "tqdm == 4.67.1\n",
    "\n",
    "pip install rawpy==0.25.0 # 만약 .dng와 같이 raw file을 .jpg와 같은 파일 형식으로 convert하려면 필요.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c7af4e",
   "metadata": {},
   "source": [
    "## 2. Parameters\n",
    "아래는 지금부터 실행할 모든 파일에서 사용되는 Parameter들이 기록되어 있고, 이를 수정할 수 있게 설정했다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efa6cbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "#################### Model Parameters #######################\n",
    "\n",
    "################### Dataset Parameters ######################\n",
    "\n",
    "parser.add_argument(\"--min_dist\", type=int, default=480, help=\"가로와 세로 중 상대적으로 짧은 곳을 480p로 설정 후, 비율에 맞게 리사이징합니다.\")\n",
    "\n",
    "#################### Training Parameters ####################\n",
    "\n",
    "parser.add_argument(\"--epoch\", type=int, default=85, help=\"start epoch\")\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=400, help=\"total epochs\")\n",
    "parser.add_argument(\"--dataset_name\", type=str, default=\"fiveK\", help=\"dataset name\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=1)\n",
    "parser.add_argument(\"--lr\", type=float, default=1e-6, help=\"Learning Rate\")\n",
    "parser.add_argument(\"--b1\", type=float, default=0.9)\n",
    "parser.add_argument(\"--b2\", type=float, default=0.999)\n",
    "parser.add_argument(\"--n_cpu\", type=int, default=12, help=\"The number of CPU threads to train model.\") \n",
    "parser.add_argument(\"--output_dir\", type=str, default=\"RGB_LUT_Classifier_Ckpt\")\n",
    "parser.add_argument(\"--lambda_ssim\", type=float, default=5, help=\"The Lambda of SSIM Loss\")\n",
    "parser.add_argument(\"--lambda_mse\", type=float, default = 10, help=\"The Lambda of MSE Loss\")\n",
    "parser.add_argument(\"--lambda_color\", type=float, default=1, help=\"The Lambda of CIEDE2000 Loss\")\n",
    "parser.add_argument(\"--2000_eps_value\", type=float, default=3e-4, help=\"일부 이미지에서 NaN이 뜨는 현상 방지를 위한 eps 값 설정. 가급적이면 1e-4 이상을 강력히 권장합니다.\")\n",
    "parser.add_argument(\"--lambda_smooth\", type=float, default=0.01, help=\"smooth regularization\")\n",
    "parser.add_argument(\"--lambda_monotonicity\", type=float, default=0.01, help=\"monotonicity regularization\")\n",
    "parser.add_argument(\"--norm\", type=str, default=\"instance\", choices=[\"instance\",\"batch\",\"none\"], help=\"normalization type used in Classifier\")\n",
    "\n",
    "#################### Evaluation Parameters ####################\n",
    "\n",
    "parser.add_argument(\"--inf_epoch\", type=int, default=22)\n",
    "parser.add_argument(\"--model_dir\", type=str, default=\"RGB_LUT_Classifier_Ckpt\")\n",
    "parser.add_argument(\"--num_workers_eval\", type=int, default=0)  # Windows면 0 권장\n",
    "\n",
    "\n",
    "\n",
    "opt, _= parser.parse_known_args()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d69e53f",
   "metadata": {},
   "source": [
    "## 3. AI model structure\n",
    "이 항목에서는 AI model을 구성하는데 필요한 코드들이 적혀있다. LUT, Trilinear, Total Variation Loss 및 Monotonic Loss 등 여러 기능을 관리할 수 있다.\n",
    "\n",
    "바로 아래에는 기본 weights 설정과 디버깅 관련 코드, 모드에 따른 Normalization을 선택할 수 있는 코드이다. 가급적이면 normalization은 건드리지 말 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbc91806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "import kornia.color as K\n",
    "import trilinear_c__ext as trilinear_ext\n",
    "\n",
    "# ---------------------- normalization2D 관련 ---------------------- \n",
    "\n",
    "def get_norm2d(num_features: int, norm_type: str = \"instance\"):\n",
    "\n",
    "    if norm_type == \"batch\":\n",
    "        return nn.BatchNorm2d(num_features)  # running stats ON by default\n",
    "    elif norm_type == \"instance\":\n",
    "        return nn.InstanceNorm2d(num_features, affine=True, track_running_stats=False)\n",
    "    elif norm_type == \"none\":\n",
    "        return nn.Identity()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported norm_type: {norm_type}\")\n",
    "\n",
    "\n",
    "# ---------------------- Debugging 및 Parameter ---------------------- \n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "\n",
    "# ----------------------  Weight 관련 ---------------------- \n",
    "\n",
    "def weights_init_normal_classifier(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.xavier_normal_(m.weight.data)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1 or classname.find(\"InstanceNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ce9035",
   "metadata": {},
   "source": [
    "아래에는 Trilinear Interpolation에 관련된 Class들이 정의되어 있습니다. 기존 코드와 가장 큰 차이점들이라면 메모리의 contiguity를 보장하여 최신 Pytorch 및 Cuda에 대응할 수 있게 수정한 것이며, 나머지는 정적 메소드 @staticmethod를 사용하여 parameter들의 상속과 관련된 문제를 해결했다는 것입니다.\n",
    "\n",
    "이 함수는 Parameter들을 지정하고 c++ 파일에서 정의된 trilinear_ext 모듈을 사용하여  연산을 수행합니다. 여기서 RGB pixel이 LUT에 Mapping되며,\n",
    "backward 단에서는 forward 단에서 Parameter들을 받고, grad_LUT = torch.zeros_like를 이용하여 미리 gradient 값을 0으로 초기화합니다. 이후 trilinear_ext 모듈을 사용하여 LUT들의 Gradient을 구하는 연산을 수행합니다. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a85bcc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrilinearInterpolation(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, LUT, x):  # 3D LUT\n",
    "        x_cont   = x.contiguous()\n",
    "        LUT_cont = LUT.contiguous()\n",
    "\n",
    "        B, C, H, W = x_cont.shape\n",
    "        dim     = LUT_cont.size(1)\n",
    "\n",
    "        shift   = dim ** 3\n",
    "        binsize = 1.0 / (dim - 1)\n",
    "\n",
    "        # CUDA 바인딩 호출 (qforward)\n",
    "        output = trilinear_ext.forward(\n",
    "            LUT_cont, x_cont,\n",
    "            dim, shift, binsize,\n",
    "            W, H, B\n",
    "        )\n",
    "\n",
    "        # backward용 저장\n",
    "        ctx.save_for_backward(x_cont, LUT_cont)\n",
    "        ctx.dim      = dim\n",
    "        ctx.shift    = shift\n",
    "        ctx.binsize  = binsize\n",
    "        ctx.W        = W\n",
    "        ctx.H        = H\n",
    "        ctx.B        = B\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, LUT = ctx.saved_tensors\n",
    "        dim      = ctx.dim\n",
    "        shift    = ctx.shift\n",
    "        binsize  = ctx.binsize\n",
    "        W, H, B= ctx.W, ctx.H, ctx.B\n",
    "\n",
    "        grad_output = grad_output.contiguous()\n",
    "\n",
    "        # backward는 LUT에 대한 그래디언트만 계산해 반환합니다: (3, D, D, D, DN)\n",
    "        grad_LUT = trilinear_ext.backward(\n",
    "            x, grad_output,\n",
    "            dim, shift, binsize,\n",
    "            W, H, B\n",
    "        )\n",
    "\n",
    "        # 입력 x에 대한 grad는 계산하지 않으면 None\n",
    "        return grad_LUT, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaf3a98",
   "metadata": {},
   "source": [
    "아래에는 3D LUT의 특성을 사전 정의하는 Class가 적혀있는 code 및, Basis 3D LUT를 위한 Zero로 세팅된 LUT를 생성하는 code가 적혀있습니다.\n",
    "\n",
    "LUT0을 담당하는 Identity의 경우에는 dimension 수에 따라 IdentityLUTxx.txt 파일을 읽어와 사전 정의된 색 공간을 형성합니다.\n",
    "LUT1, LUT2의 경우에는 처음에는 3차원 영벡터로부터 시작합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc04690c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator3DLUT_identity(nn.Module):\n",
    "    def __init__(self, dim=33):\n",
    "        super(Generator3DLUT_identity, self).__init__()\n",
    "        fname = \"IdentityLUT33.txt\" if dim==33 else \"IdentityLUT64.txt\"\n",
    "        with open(fname,'r') as file:\n",
    "            LUT_lines = file.readlines()\n",
    "        LUT = torch.zeros(3,dim,dim,dim)\n",
    "        for i in range(dim):\n",
    "            for j in range(dim):\n",
    "                for k in range(dim):\n",
    "                    n = i*dim*dim + j*dim + k\n",
    "                    x = LUT_lines[n].split()\n",
    "                    LUT[0,i,j,k] = float(x[0])\n",
    "                    LUT[1,i,j,k] = float(x[1])\n",
    "                    LUT[2,i,j,k] = float(x[2])\n",
    "        self.LUT = nn.Parameter(LUT)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return TrilinearInterpolation.apply(self.LUT, x)\n",
    "\n",
    "\n",
    "class Generator3DLUT_zero(nn.Module):\n",
    "    def __init__(self, dim=33):\n",
    "        super(Generator3DLUT_zero, self).__init__()\n",
    "        LUT = torch.zeros(3,dim,dim,dim)\n",
    "        self.LUT = nn.Parameter(LUT)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return TrilinearInterpolation.apply(self.LUT, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e399547b",
   "metadata": {},
   "source": [
    "아래에는 Classifier에 관련된 Class들이 모여있습니다. Classifier Class은 주어진 데이터들을 먼저 256*256으로 Upsampling하는 과정을 거치며, 이를 Conv2d를 통해 convolution 연산을 시행시킨 후, \n",
    "LeakyReLU 연산 후 이를 discriminator_block울 통해 한 번 더 RGB에 대해 normalization을 시킵니다. 크기를 2배씩 증가시키며 반복 계산 후, 다시 원래 Channel 수인 3으로 convolution 연산을 수행합니다.\n",
    "\n",
    "discriminator_block의 경우에는 Layer를 정의하는 함수이며, Conv2d와 LeakyReLU 함수로 Layer가 정의되어 있습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9cb288a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module): ##weight predictor model \n",
    "    def __init__(self, in_channels=3, num_LUTS=3):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Upsample(size=(256,256), mode='bilinear'),\n",
    "            nn.Conv2d(in_channels ,16,3,stride=2,padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.InstanceNorm2d(16, affine=True),\n",
    "            *discriminator_block(16,32, normalization=True),\n",
    "            *discriminator_block(32,64, normalization=True),\n",
    "            *discriminator_block(64,128, normalization=True),\n",
    "            *discriminator_block(128,128),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Conv2d(128,num_LUTS,8)\n",
    "        )\n",
    "\n",
    "    def forward(self, img_input):\n",
    "        return self.model(img_input)\n",
    "    \n",
    "def discriminator_block(in_filters, out_filters, normalization=False):\n",
    "    layers = [nn.Conv2d(in_filters,out_filters,3,stride=2,padding=1),\n",
    "              nn.LeakyReLU(0.2)]\n",
    "    if normalization:\n",
    "        layers.append(nn.InstanceNorm2d(out_filters, affine=True))\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa6ffa6",
   "metadata": {},
   "source": [
    "아래에는 Monotonic Loss 및 Smooth Loss를 정의한 TV_3D가 포함되어 있습니다. 해당 코드의 forward를 살펴보면, LUT를 통과시킨 Tensor에서 Channel을 제외한 3개의 차원을 기준으로, 양 옆의 픽셀 값의 차이를 이용하여 tv항의 경우 __init__에서 정의한 self.weights와 적절히 곱해져, 그것의 평균값을 tv항의 출력으로, mn항의 경우 ReLU 함수를 통과시켜, 그것의 평균값을 mn항의 출력으로 정의하였습니다.\n",
    "\n",
    "해당 코드는 적절한 파라미터 설정 시 False Contour 현상을 줄일 수 있고, 픽셀별로 색상의 경계가 부자연스럽게 생성되는 것을 어느 정도 완화해줍니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5b895fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TV_3D(nn.Module):\n",
    "    def __init__(self, dim=33):\n",
    "        super(TV_3D, self).__init__()\n",
    "        # Register weights as buffers so .to(device) moves them automatically\n",
    "        w_r = torch.ones(3, dim, dim, dim-1)\n",
    "        w_r[:,:,:,(0,dim-2)] *= 2.0\n",
    "        self.register_buffer('weight_r', w_r)\n",
    "\n",
    "        w_g = torch.ones(3, dim, dim-1, dim)\n",
    "        w_g[:,:,(0,dim-2),:] *= 2.0\n",
    "        self.register_buffer('weight_g', w_g)\n",
    "\n",
    "        w_b = torch.ones(3, dim-1, dim, dim)\n",
    "        w_b[:,(0,dim-2),:,:] *= 2.0\n",
    "        self.register_buffer('weight_b', w_b)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, lut_tensor):\n",
    "        # Accept raw LUT tensor rather than module\n",
    "        LUT = lut_tensor if not hasattr(lut_tensor, 'LUT') else lut_tensor.LUT\n",
    "        #    weight_r.shape[0] == C (e.g. 3)\n",
    "        if LUT.shape[0] != self.weight_r.shape[0]:\n",
    "            # assume it’s [D, C, H, W] → move C→0\n",
    "            LUT = LUT.permute(1, 0, 2, 3).contiguous()\n",
    "\n",
    "        dif_r = LUT[:,:,:,:-1] - LUT[:,:,:,1:]\n",
    "        dif_g = LUT[:,:,:-1,:] - LUT[:,:,1:,:]\n",
    "        dif_b = LUT[:,:-1,:,:] - LUT[:,1:,:,:]\n",
    "        tv = (torch.mean(dif_r**2 * self.weight_r) +\n",
    "              torch.mean(dif_g**2 * self.weight_g) +\n",
    "              torch.mean(dif_b**2 * self.weight_b))\n",
    "        mn = (torch.mean(self.relu(dif_r)) +\n",
    "              torch.mean(self.relu(dif_g)) +\n",
    "              torch.mean(self.relu(dif_b)))\n",
    "        return tv, mn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f382aa50",
   "metadata": {},
   "source": [
    "## 4. Image Dataset \n",
    "해당 항목은 Image의 Dataset과 관련된 코드가 작성된 항목입니다. sRGB Paired 상황을 가정하고 있기 때문에, Paired 부분만 기록했습니다.\n",
    "ImageDataset_sRGB class를 지정하여, input 및 Target image 파일들의 list를 이용하여 image들을 불러옵니다. \n",
    "불러온 사진을 RGB로 변환하여, 이를 Tensor 형태로 넘깁니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8348271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ──────────────── 사전 공통 함수 ────────────────\n",
    "def read_name_list(txt_path: Path):\n",
    "    \"\"\"TXT 한 줄에 하나씩, 파일명만 (확장자 포함) 리턴.\"\"\"\n",
    "    return [line.strip() for line in txt_path.read_text(encoding=\"utf-8\").splitlines() if line.strip()]\n",
    "\n",
    "def build_paths(root: Path, subdir: str, names: list):\n",
    "    \"\"\"root/subdir 에 name 을 붙여 Path 객체 리스트로 반환.\"\"\"\n",
    "    base = root / subdir\n",
    "    paths = [base / name for name in names]\n",
    "    # 존재 여부 체크\n",
    "    for p in paths:\n",
    "        if not p.exists():\n",
    "            raise FileNotFoundError(f\"File not found: {p}\")\n",
    "    return paths\n",
    "\n",
    "\n",
    "# 2025. 08. 05 sRGB Paired만 수정함.\n",
    "\n",
    "# ──────────────── sRGB Paired ────────────────\n",
    "class ImageDataset_sRGB(Dataset):\n",
    "    def __init__(self, root, mode=\"train\"):\n",
    "        self.root = Path(root)\n",
    "        self.mode = mode\n",
    "\n",
    "        # 읽을 TXT 파일 경로\n",
    "        input_list = self.root / \"input_list.txt\"\n",
    "        Target_list = self.root / \"correct.txt\"\n",
    "        test_list  = self.root / \"test.txt\"\n",
    "\n",
    "        # train 모드\n",
    "        if mode == \"train\":\n",
    "            names_inp = read_name_list(input_list)\n",
    "            names_inp_tar = read_name_list(Target_list)\n",
    "            self.input_files      = build_paths(self.root, \"input/JPG/\", names_inp)\n",
    "            self.target_files     = build_paths(self.root, \"Target/JPG\", names_inp_tar)\n",
    "        else:  # test 모드\n",
    "            names_test    = read_name_list(test_list)\n",
    "            self.input_files     = build_paths(self.root, \"input/JPG\", names_test)\n",
    "            self.target_files    = build_paths(self.root, \"inference_target/JPG\", names_test)\n",
    "\n",
    "        # transforms / to_tensor\n",
    "        self.to_tensor = TF.to_tensor\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_files)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inp_path = self.input_files[idx]\n",
    "\n",
    "        img_in  = Image.open(inp_path).convert(\"RGB\")\n",
    "        W, H = img_in.size\n",
    "\n",
    "        if W >= H:\n",
    "            H_new = opt.min_dist\n",
    "            W_new = int(round(W * H_new / H))\n",
    "        else:\n",
    "            W_new = opt.min_dist\n",
    "            H_new = int(round(H * W_new / W))\n",
    "\n",
    "        img_in = img_in.resize((W_new, H_new))\n",
    "\n",
    "        \n",
    "        t_in  = self.to_tensor(img_in)   # [0,1]\n",
    "\n",
    "        if self.mode == \"train\":\n",
    "            tgt_path = self.target_files[idx]\n",
    "            img_tgt = Image.open(tgt_path).convert(\"RGB\")\n",
    "\n",
    "            img_tgt = img_tgt.resize((W_new, H_new))\n",
    "            \n",
    "            t_tgt = self.to_tensor(img_tgt)\n",
    "\n",
    "            return {\n",
    "                \"A_input\":  t_in,\n",
    "                \"A_target\": t_tgt,\n",
    "                \"input_name\": inp_path.name\n",
    "            }\n",
    "\n",
    "        else:  \n",
    "            tgt_path = self.target_files[idx]\n",
    "            img_tgt = Image.open(tgt_path).convert(\"RGB\")\n",
    "\n",
    "            img_tgt = img_tgt.resize((W_new, H_new))\n",
    "            \n",
    "            t_tgt = self.to_tensor(img_tgt)# test\n",
    "            return {\n",
    "                \"A_input\":  t_in,\n",
    "                \"A_target\": t_tgt,\n",
    "                \"input_name\": inp_path.name\n",
    "            }\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaece9b",
   "metadata": {},
   "source": [
    "## 5. Loss Function 및 Optimizer\n",
    "Loss Function 및 Optimizer와 관련된 class가 정의되어 있는 항목입니다.\n",
    "\n",
    "Optimizer는 Adam을 사용하고, Classifier의 학습률과 LUT들의 학습률을 서로 다르게 설정할 수 있게 해놔 표본 수가 적은 상황에서도 학습률을 높여 더 빠르게 결과물을 도출할 수 있게 만들었습니다.\n",
    "\n",
    "Loss Function의 경우에는 새로 구축하여, 세부 항목으로 분리하겠습니다.\n",
    "\n",
    "학습률의 경우 너무 높게 설정할 시 PSNR 값 등이 수렴하지 않고 진동하거나, 혹은 epoch가 지날 수록 학습 수준이 향상하는 것이 아닌 오히려 퇴보할 수 있으므로 적절하게 조절하시길 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f94082b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --- Data loader ---\n",
    "Dataset = ImageDataset_sRGB\n",
    "train_dataset = Dataset(f\"data/{opt.dataset_name}\", mode=\"train\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# persistent_workers는 num_workers>0일 때만 안전\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                            batch_size=opt.batch_size, shuffle=True,\n",
    "                            num_workers=opt.n_cpu)\n",
    "\n",
    "LUT0 = Generator3DLUT_identity(dim=33).to(device)\n",
    "LUT1 = Generator3DLUT_zero().to(device)\n",
    "LUT2 = Generator3DLUT_zero().to(device)\n",
    "classifier = Classifier().to(device)\n",
    "\n",
    "\n",
    "# --- Model & Optimizer ---\n",
    "optimizer_G = torch.optim.Adam(itertools.chain(classifier.parameters(), LUT0.parameters(), LUT1.parameters(), LUT2.parameters()),\n",
    "    lr=opt.lr, betas=(opt.b1, opt.b2)\n",
    ")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0021e3",
   "metadata": {},
   "source": [
    "### 5.1. MSE Loss\n",
    "새로 수정한 Loss에서는, MSE Loss가 여전히 포함되어 있습니다.\n",
    "Pytorch에서 제공하는 MSE Loss를 이용하고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d241c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "# mse_psnr = F.mse_loss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c875a4c7",
   "metadata": {},
   "source": [
    "### 5.2 Total Variation Loss (TV_3D)\n",
    "상단에서 정의했던 TV_3D 함수를 호출하는 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e3d2645",
   "metadata": {},
   "outputs": [],
   "source": [
    "TV3 = TV_3D().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ec1e22",
   "metadata": {},
   "source": [
    "### 5.3 SSIM Loss\n",
    "원래 SSIM은 평가 지표 중 하나로써, 두 사진이 얼마나 유시한지를 나타내는 함수입니다.\n",
    "서로 유사할 경우 1에 가까운 값을, 서로 유사하지 않을 경우 0에 가까운 값이 출력되는 경향을 보입니다. 이는 Loss 값이 높으면 유사하지 않아야 하는 것과 대치됩니다.\n",
    "따라서, 저희는 (1-SSIM)을 SSIM Loss로 정의하여, Loss로써의 기능을 할 수 있도록 구현했습니다.\n",
    "해당 함수는 Pytorch에서 제공하는 nn.AvgPool2d를 이용하여, 주변 픽셀의 평균을 도출하여 이를 Loss 계산에 이용합니다. 이 과정에서 손실되는 가장자리 픽셀들은 nn.ReflectionPad2d를 이용하여 input image의 손실을 방지합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1242e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSIM(nn.Module):\n",
    "    \"\"\"Layer to compute the SSIM loss between a pair of images\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SSIM, self).__init__()\n",
    "        self.mu_x_pool   = nn.AvgPool2d(3, 1)\n",
    "        self.mu_y_pool   = nn.AvgPool2d(3, 1)\n",
    "        self.sig_x_pool  = nn.AvgPool2d(3, 1)\n",
    "        self.sig_y_pool  = nn.AvgPool2d(3, 1)\n",
    "        self.sig_xy_pool = nn.AvgPool2d(3, 1)\n",
    "\n",
    "        # 입력 경계의 반사를 사용하여 상/하/좌/우에 입력 텐서를 추가로 채웁니다.\n",
    "        self.refl = nn.ReflectionPad2d(1)\n",
    "\n",
    "        self.C1 = 0.01 ** 2\n",
    "        self.C2 = 0.03 ** 2\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # shape : (xh, xw) -> (xh + 2, xw + 2)\n",
    "        x = self.refl(x) \n",
    "        # shape : (yh, yw) -> (yh + 2, yw + 2)\n",
    "        y = self.refl(y)\n",
    "\n",
    "        mu_x = self.mu_x_pool(x)\n",
    "        mu_y = self.mu_y_pool(y)\n",
    "\n",
    "        sigma_x  = self.sig_x_pool(x ** 2) - mu_x ** 2\n",
    "        sigma_y  = self.sig_y_pool(y ** 2) - mu_y ** 2\n",
    "        sigma_xy = self.sig_xy_pool(x * y) - mu_x * mu_y\n",
    "\n",
    "        SSIM_n = (2 * mu_x * mu_y + self.C1) * (2 * sigma_xy + self.C2)\n",
    "        SSIM_d = (mu_x ** 2 + mu_y ** 2 + self.C1) * (sigma_x + sigma_y + self.C2)\n",
    "\n",
    "        return torch.clamp((1 - SSIM_n / SSIM_d), 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b4af94",
   "metadata": {},
   "source": [
    "### 5.4 CIEDE 2000 Loss\n",
    "Lab 색공간에서 두 색깔이 서로 얼마나 떨어져 있는가를 정의한 CIEDE 2000 색차식을 이용하여, 두 색깔 간의 거리를 사용하여 Loss로 이용하고 있습니다.\n",
    "먼저, input image들이 sRGB 공간에서 정의되어 있기에, 이것을 XYZ 색공간으로 바꾼 후, 다시 Lab 색공간으로 바꿔줘야 합니다.\n",
    "이후, 색차식에 따라 Delta E (두 색깔의 거리)를 계산하여 출력하는 함수입니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a85857b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "import numpy as np\n",
    "\n",
    "# ----- sRGB <-> linear -----\n",
    "def srgb_to_linear(x: torch.Tensor) -> torch.Tensor:\n",
    "    a = 0.055\n",
    "    x = x.clamp(0.0, 1.0)\n",
    "    return torch.where(x <= 0.04045, x/12.92, ((x + a)/1.055) ** 2.4)\n",
    "\n",
    "# def linear_to_srgb(x: torch.Tensor) -> torch.Tensor:\n",
    "#     a = 0.055\n",
    "#     x = x.clamp(0.0, 1.0)\n",
    "#     return torch.where(x <= 0.0031308, 12.92 * x, 1.055 * torch.pow(x, 1/2.4) - a)\n",
    "\n",
    "    # ----- linear RGB(NCHW) -> XYZ(NCHW) (sRGB, D65) -----\n",
    "def rgb_to_xyz_linear(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    x: [B,3,H,W] linear RGB in [0,1]\n",
    "    returns XYZ: [B,3,H,W]\n",
    "    \"\"\"\n",
    "    M = x.new_tensor([[0.4124564, 0.3575761, 0.1804375],\n",
    "                    [0.2126729, 0.7151522, 0.0721750],\n",
    "                    [0.0193339, 0.1191920, 0.9503041]])     # sRGB->XYZ(D65)\n",
    "    B, C, H, W = x.shape\n",
    "    x_flat = x.permute(0, 2, 3, 1).reshape(-1, 3)             # [N,3]\n",
    "    xyz = x_flat @ M.t()    # [N,3] \n",
    "    return xyz.reshape(B, H, W, 3).permute(0, 3, 1, 2)      # [B,3,H,W]\n",
    "\n",
    "    # ----- XYZ(NCHW) -> Lab(NCHW) -----\n",
    "def xyz_to_lab(xyz: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    xyz: [B,3,H,W], same scale as white (here 0..1)\n",
    "    returns Lab: [B,3,H,W] with L in [0,100] approx\n",
    "    \"\"\"\n",
    "    X, Y, Z = xyz[:, 0], xyz[:, 1], xyz[:, 2]\n",
    "    Xn, Yn, Zn = xyz.new_tensor(0.95047), xyz.new_tensor(1.0), xyz.new_tensor(1.08883)  # D65\n",
    "    xr, yr, zr = X / Xn, Y / Yn, Z / Zn\n",
    "\n",
    "    delta = 6/29\n",
    "    th = delta**3\n",
    "    def f(t):  # piecewise cube-root\n",
    "        return torch.where(t > th, (t.pow(1/3)+ 3e-4) , t/(3*delta*delta) + 4/29)\n",
    "\n",
    "    fx, fy, fz = f(xr), f(yr), f(zr)\n",
    "    L = 116*fy - 16\n",
    "    a = 500*(fx - fy)\n",
    "    b = 200*(fy - fz)\n",
    "    return torch.stack([L, a, b], dim=1)\n",
    "\n",
    "def torch_ciede2000_loss_stable(lab_true, lab_pred):\n",
    "    # Pytorch는 constant와 비슷한 역할을 하는 함수가 없음.\n",
    "    PI = np.pi\n",
    "    TWO_PI = 2.0*np.pi\n",
    "    EPS = 1e-8\n",
    "    BIG_EPS = 1e-6 # 분모/루트용 좀 더 크게\n",
    "\n",
    "    if lab_true.dim() == 4 and lab_true.size(1) == 3:   # NCHW\n",
    "        L1,a1,b1 = lab_true[:,0], lab_true[:,1], lab_true[:,2]\n",
    "        L2,a2,b2 = lab_pred[:,0], lab_pred[:,1], lab_pred[:,2]\n",
    "    elif lab_true.shape[-1] == 3:                   # ...C last\n",
    "        L1,a1,b1 = lab_true[...,0], lab_true[...,1], lab_true[...,2]\n",
    "        L2,a2,b2 = lab_pred[...,0], lab_pred[...,1], lab_pred[...,2]\n",
    "    else:\n",
    "        raise ValueError(\"lab tensors must have a size-3 channel\")\n",
    "\n",
    "    # (옵션) Lab 예측값을 물리적 범위로 부드럽게 제한: tanh 스케일링\n",
    "    # 완전 hard-clip은 비미분점이 생기므로 tanh 권장\n",
    "    # L2 = 50.0*(torch.tanh(L2/50.0))+50.0         # 대략 [0,100] 근처\n",
    "    # a2 = 128.0*torch.tanh(a2/128.0)              # 대략 [-128,128]\n",
    "    # b2 = 128.0*torch.tanh(b2/128.0)\n",
    "\n",
    "\n",
    "    # 1) 기본 Chroma\n",
    "    C1 = torch.sqrt(a1*a1 + b1*b1 + EPS)\n",
    "    C2 = torch.sqrt(a2*a2 + b2*b2 + EPS)\n",
    "    C_bar = 0.5*(C1 + C2)\n",
    "    # 2) pow7 안정화: base를 먼저 제한해 overflow 자체를 방지\n",
    "    # float32에서 (1e30)^(1/7) ≈ 2e4 정도. 여유롭게 1e4~2e4 사이로 제한.\n",
    "    POW7_BASE_LIMIT = 2e4\n",
    "    def pow7_safe(x):\n",
    "        x_clip = torch.minimum(torch.maximum(x, torch.zeros_like(x, device=device)), POW7_BASE_LIMIT * torch.ones_like(x, device=device))\n",
    "        return torch.pow(x_clip, 7.0)\n",
    "\n",
    "    num = pow7_safe(C_bar)\n",
    "    den = num + 25.0**7\n",
    "    # 분수는 항상 [0,1]로 clamp\n",
    "    frac = num / torch.maximum(den, BIG_EPS*torch.ones_like(den, device=device))\n",
    "    frac = torch.clamp(frac, 0.0, 1.0)\n",
    "\n",
    "    G = 0.5 * (1.0 - torch.sqrt(frac))\n",
    "\n",
    "    a1p = (1.0 + G) * a1\n",
    "    a2p = (1.0 + G) * a2\n",
    "    C1p = torch.sqrt(a1p*a1p + b1*b1 + EPS)\n",
    "    C2p = torch.sqrt(a2p*a2p + b2*b2 + EPS)\n",
    "\n",
    "    # C'≈0이면 hue는 정의되지 않으므로 표준 특례: h'을 0으로 두고 이후 항이 영향 거의 없도록 함\n",
    "    C1p_nz = C1p > BIG_EPS\n",
    "    C2p_nz = C2p > BIG_EPS\n",
    "\n",
    "    h1p_raw = torch.atan2(b1, a1p + EPS)  # 분모 EPS\n",
    "    h2p_raw = torch.atan2(b2, a2p + EPS)\n",
    "    h1p_raw = torch.where(h1p_raw < 0, h1p_raw + TWO_PI, h1p_raw)\n",
    "    h2p_raw = torch.where(h2p_raw < 0, h2p_raw + TWO_PI, h2p_raw)\n",
    "\n",
    "    h1p = torch.where(C1p_nz, h1p_raw, torch.zeros_like(h1p_raw, device=device))\n",
    "    h2p = torch.where(C2p_nz, h2p_raw, torch.zeros_like(h2p_raw, device=device))\n",
    "\n",
    "    # 3) deltas\n",
    "    dLp = L2 - L1\n",
    "    dCp = C2p - C1p\n",
    "\n",
    "    hdiff = h2p - h1p\n",
    "    hdiff = torch.where(hdiff >  PI, hdiff - TWO_PI, hdiff)\n",
    "    hdiff = torch.where(hdiff < -PI, hdiff + TWO_PI, hdiff)\n",
    "\n",
    "    # mult = sqrt(C1p*C2p) -> 0에서 기울기 발산 방지\n",
    "    mult = torch.sqrt(torch.maximum(C1p*C2p, BIG_EPS*torch.ones_like(C1p*C2p)))\n",
    "    dHp = 2.0 * mult * torch.sin(0.5 * hdiff)\n",
    "    # 표준 특례: 둘 중 하나라도 C'≈0이면 dH'≈0\n",
    "    dHp = torch.where(C1p_nz & C2p_nz, dHp, torch.zeros_like(dHp, device=device))\n",
    "\n",
    "    # 4) 가중치/회전항\n",
    "    Lbp = 0.5 * (L1 + L2)\n",
    "    Cbp = 0.5 * (C1p + C2p)\n",
    "\n",
    "    # hue 평균도 특례 필요: 두 쪽 모두 C' > 0일 때만 의미\n",
    "    hb_same = torch.abs(h1p - h2p) <= PI\n",
    "    hbp_raw = torch.where(hb_same, 0.5*(h1p + h2p), 0.5*(h1p + h2p + TWO_PI))\n",
    "    hbp = torch.where(C1p_nz & C2p_nz, hbp_raw, BIG_EPS*torch.ones_like(hbp_raw, device=device))\n",
    "\n",
    "    # T\n",
    "    T = (1.0\n",
    "        - 0.17 * torch.cos(hbp - PI/6.0)\n",
    "        + 0.24 * torch.cos(2.0*hbp)\n",
    "        + 0.32 * torch.cos(3.0*hbp + PI/30.0)\n",
    "        - 0.20 * torch.cos(4.0*hbp - 63.0*np.pi/180.0))\n",
    "    T = torch.clamp(T, -2.0, 2.0)\n",
    "\n",
    "    delta_theta = 30.0*np.pi/180.0 * torch.exp(-torch.square((hbp*180.0/PI - 275.0) / 25.0))\n",
    "\n",
    "    RC_num = pow7_safe(Cbp)\n",
    "    RC = 2.0 * torch.sqrt(torch.clamp(RC_num / torch.maximum(RC_num + 25.0**7, BIG_EPS*torch.ones_like(RC_num)), 0.0, 1.0))\n",
    "    RT = -torch.sin(2.0 * delta_theta) * RC\n",
    "    RT = torch.clamp(RT, -1.5, 1.5)\n",
    "\n",
    "    SL = 1.0 + (0.015 * torch.square(Lbp - 50.0)) / torch.sqrt(20.0 + torch.square(Lbp - 50.0))\n",
    "    SC = 1.0 + 0.045 * Cbp\n",
    "    SH = 1.0 + 0.015 * Cbp * T\n",
    "    SL = torch.maximum(SL, BIG_EPS*torch.ones_like(SL)); SC = torch.maximum(SC, BIG_EPS*torch.ones_like(SC)); SH = torch.maximum(SH, BIG_EPS*torch.ones_like(SH))\n",
    "\n",
    "    kL = kC = kH = 1.0\n",
    "    termL = torch.square(dLp / (kL * SL))\n",
    "    termC = torch.square(dCp / (kC * SC))\n",
    "    termH = torch.square(dHp / (kH * SH))\n",
    "\n",
    "    cross = RT * (dCp / (kC * SC)) * (dHp / (kH * SH)) + 1e-9\n",
    "    # cross도 폭이 크면 clamp\n",
    "    cross = torch.clamp(cross, -1e6, 1e6)\n",
    "\n",
    "    rad = termL + termC + termH + cross\n",
    "\n",
    "    # (선택 1) sqrt를 쓰면 0 근처 gradient가 날카롭습니다 → EPS 추가\n",
    "    # dE = tf.sqrt(tf.maximum(rad, 0.0) + BIG_EPS)\n",
    "\n",
    "    # (선택 2, 추천) deltaE^2를 loss로 사용 (미분 더 안정)\n",
    "    dE2 = torch.maximum(rad, torch.zeros_like(rad, device=device))\n",
    "\n",
    "    # forward finite 체크 (학습 중엔 끄는 것도 고려)\n",
    "    # tf.debugging.assert_all_finite(dE2, \"deltaE2000^2\")\n",
    "    dE2 = torch.sqrt(dE2)\n",
    "\n",
    "    # 평균 반환\n",
    "    return torch.mean(dE2)  # 또는 tf.reduce_mean(dE) if sqrt 버전 사용\n",
    "\n",
    "\n",
    "    # ----- Convenience: ΔE00 directly from sRGB tensors -----\n",
    "def delta_e00_from_srgb(img1: torch.Tensor, img2: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    img1, img2: sRGB in [0,1], shape [B,3,H,W] (NCHW)\n",
    "    returns ΔE00: [B,H,W]\n",
    "    \"\"\"\n",
    "    lin1 = srgb_to_linear(img1)\n",
    "    lin2 = srgb_to_linear(img2)\n",
    "    xyz1 = rgb_to_xyz_linear(lin1)\n",
    "    xyz2 = rgb_to_xyz_linear(lin2)\n",
    "    lab1 = xyz_to_lab(xyz1)       # [B,3,H,W]\n",
    "    lab2 = xyz_to_lab(xyz2)\n",
    "    return torch_ciede2000_loss_stable(lab1, lab2)  # [B,H,W]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb04fef",
   "metadata": {},
   "source": [
    "## 6. matplotlib Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8502bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from typing import Optional, List\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class RealTimePlotter:\n",
    "    def __init__(\n",
    "        self,\n",
    "        title: str = \"Training Metrics\",\n",
    "        max_points: int = 1000,\n",
    "        ema_alpha: Optional[float] = None,  # e.g., 0.2 for smoothing; None disables EMA\n",
    "        figsize=(8, 6),\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            title: Figure title\n",
    "            max_points: keep at most this many recent points in memory/plot\n",
    "            ema_alpha: if not None, also plot an EMA-smoothed curve with coefficient alpha\n",
    "            figsize: matplotlib figure size\n",
    "        \"\"\"\n",
    "        # Enable interactive mode\n",
    "        plt.ion()\n",
    "\n",
    "        self.title = title\n",
    "        self.max_points = max_points\n",
    "        self.ema_alpha = ema_alpha\n",
    "\n",
    "        # Data buffers\n",
    "        self.epochs = deque(maxlen=max_points)\n",
    "        self.psnr = deque(maxlen=max_points)\n",
    "        self.loss = deque(maxlen=max_points)\n",
    "        self.psnr_ema = None\n",
    "        self.loss_ema = None\n",
    "\n",
    "        # Figure & axes\n",
    "        self.fig = plt.figure(figsize=figsize)\n",
    "        self.fig.suptitle(self.title)\n",
    "        # Two separate axes stacked vertically\n",
    "        self.ax_psnr = self.fig.add_subplot(2, 1, 1)\n",
    "        self.ax_loss = self.fig.add_subplot(2, 1, 2)\n",
    "\n",
    "        self.ax_psnr.set_xlabel(\"Epoch\")\n",
    "        self.ax_psnr.set_ylabel(\"PSNR (dB)\")\n",
    "        self.ax_loss.set_xlabel(\"Epoch\")\n",
    "        self.ax_loss.set_ylabel(\"Loss\")\n",
    "\n",
    "        # Lines (for fast updates, keep references)\n",
    "        (self.line_psnr,) = self.ax_psnr.plot([], [], lw=1.5, label=\"PSNR\")\n",
    "        (self.line_loss,) = self.ax_loss.plot([], [], lw=1.5, label=\"Loss\")\n",
    "\n",
    "        # Optional EMA lines\n",
    "        self.line_psnr_ema = None\n",
    "        self.line_loss_ema = None\n",
    "        if self.ema_alpha is not None:\n",
    "            (self.line_psnr_ema,) = self.ax_psnr.plot([], [], lw=1.0, linestyle=\"--\", label=f\"PSNR EMA α={self.ema_alpha}\")\n",
    "            (self.line_loss_ema,) = self.ax_loss.plot([], [], lw=1.0, linestyle=\"--\", label=f\"Loss EMA α={self.ema_alpha}\")\n",
    "            self.ax_psnr.legend(loc=\"best\")\n",
    "            self.ax_loss.legend(loc=\"best\")\n",
    "\n",
    "        # Backgrounds for blitting\n",
    "        self.fig.canvas.draw()\n",
    "        self.bg_psnr = self.fig.canvas.copy_from_bbox(self.ax_psnr.bbox)\n",
    "        self.bg_loss = self.fig.canvas.copy_from_bbox(self.ax_loss.bbox)\n",
    "\n",
    "        # Initial limits\n",
    "        self.ax_psnr.set_xlim(0, 1)\n",
    "        self.ax_psnr.set_ylim(0, 1)\n",
    "        self.ax_loss.set_xlim(0, 1)\n",
    "        self.ax_loss.set_ylim(0, 1)\n",
    "\n",
    "        # Make sure everything is drawn the first time\n",
    "        self.fig.canvas.flush_events()\n",
    "\n",
    "    def _ema_update(self, prev, value):\n",
    "        if prev is None:\n",
    "            return value\n",
    "        return self.ema_alpha * value + (1 - self.ema_alpha) * prev\n",
    "\n",
    "    def _autoscale_axis(self, ax, xdata, ydata, pad_frac=0.05):\n",
    "        if len(xdata) == 0:\n",
    "            return\n",
    "        xmin, xmax = min(xdata), max(xdata)\n",
    "        ymin, ymax = min(ydata), max(ydata)\n",
    "\n",
    "        # Avoid zero range\n",
    "        if xmax == xmin:\n",
    "            xmax = xmin + 1.0\n",
    "        if ymax == ymin:\n",
    "            ymax = ymin + 1.0\n",
    "\n",
    "        # Add padding\n",
    "        xpad = (xmax - xmin) * pad_frac\n",
    "        ypad = (ymax - ymin) * pad_frac\n",
    "        ax.set_xlim(xmin - xpad, xmax + xpad)\n",
    "        ax.set_ylim(ymin - ypad, ymax + ypad)\n",
    "\n",
    "    def update(self, epoch: float, psnr_value: float, loss_value: float):\n",
    "        \"\"\"Add a new point and refresh the plot in real time.\"\"\"\n",
    "        # Append\n",
    "        self.epochs.append(epoch)\n",
    "        self.psnr.append(psnr_value)\n",
    "        self.loss.append(loss_value)\n",
    "\n",
    "        # EMA\n",
    "        if self.ema_alpha is not None:\n",
    "            self.psnr_ema = self._ema_update(self.psnr_ema, psnr_value)\n",
    "            self.loss_ema = self._ema_update(self.loss_ema, loss_value)\n",
    "\n",
    "        # Update line data\n",
    "        self.line_psnr.set_data(self.epochs, self.psnr)\n",
    "        self.line_loss.set_data(self.epochs, self.loss)\n",
    "\n",
    "        if self.ema_alpha is not None:\n",
    "            # Reconstruct EMA arrays for display (same length as epochs)\n",
    "            # We keep an incremental EMA state, but for display we'll build an array.\n",
    "            ema_psnr_series: List[float] = []\n",
    "            ema_val = None\n",
    "            for v in self.psnr:\n",
    "                ema_val = v if ema_val is None else self._ema_update(ema_val, v)\n",
    "                ema_psnr_series.append(ema_val)\n",
    "            ema_loss_series: List[float] = []\n",
    "            ema_val = None\n",
    "            for v in self.loss:\n",
    "                ema_val = v if ema_val is None else self._ema_update(ema_val, v)\n",
    "                ema_loss_series.append(ema_val)\n",
    "\n",
    "            self.line_psnr_ema.set_data(self.epochs, ema_psnr_series)\n",
    "            self.line_loss_ema.set_data(self.epochs, ema_loss_series)\n",
    "\n",
    "        # Autoscale\n",
    "        self._autoscale_axis(self.ax_psnr, self.epochs, self.psnr)\n",
    "        self._autoscale_axis(self.ax_loss, self.epochs, self.loss)\n",
    "\n",
    "        # Blitting for speed\n",
    "        self.fig.canvas.restore_region(self.bg_psnr)\n",
    "        self.fig.canvas.restore_region(self.bg_loss)\n",
    "\n",
    "        self.ax_psnr.draw_artist(self.line_psnr)\n",
    "        self.ax_loss.draw_artist(self.line_loss)\n",
    "\n",
    "        if self.ema_alpha is not None:\n",
    "            self.ax_psnr.draw_artist(self.line_psnr_ema)\n",
    "            self.ax_loss.draw_artist(self.line_loss_ema)\n",
    "\n",
    "        self.fig.canvas.blit(self.ax_psnr.bbox)\n",
    "        self.fig.canvas.blit(self.ax_loss.bbox)\n",
    "        self.fig.canvas.flush_events()\n",
    "\n",
    "    def finalize(self, save_path: Optional[str] = None):\n",
    "        \"\"\"Call at the end: optionally save a final static image, and turn off interactive mode.\"\"\"\n",
    "        # Draw a final full redraw to ensure saved figure is complete\n",
    "        self.fig.canvas.draw()\n",
    "        if save_path:\n",
    "            self.fig.savefig(save_path, bbox_inches=\"tight\", dpi=150)\n",
    "        plt.ioff()\n",
    "        plt.show(block=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386bb028",
   "metadata": {},
   "source": [
    "## 7. Training Code\n",
    "\n",
    "Training과 관련된 기능을 수행하는 code들이 적힌 항목입니다.\n",
    "\n",
    "아래에는 평가 PSNR 값을 계산하는 함수인 evaluate_psnr 함수 및 [0,1] 정규화를 위한 _to01 함수, LUT 출력과 가중치를 조합하여 결과를 출력하는 gen_train 함수가 정의되어 있으며, 학습 Loop 및 Checkpoint 파일 저장 기능을 하는 code가 작성되어 있습니다.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a0fd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 85/400] [Batch 255/4500] [psnr: 20.962425, loss: 6.701409] [mse:0.002350] ETA: 3 days, 2:17:4713"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 174\u001b[39m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    173\u001b[39m     freeze_support()\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 125\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    122\u001b[39m total_loss = opt.lambda_mse*mse_psnr + opt.lambda_smooth*tv + opt.lambda_monotonicity*mn + ssim_loss*opt.lambda_ssim + opt.lambda_color*ciede2000_loss\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# Backprop\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m \u001b[43mtotal_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m optimizer_G.step()\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# PSNR (per-batch)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import argparse, os, time, math, datetime, sys\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.multiprocessing import freeze_support\n",
    "from models import *\n",
    "from datasets import ImageDataset_sRGB\n",
    "\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# --------------------- Cuda Device 선택  ---------------------\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "plotter = RealTimePlotter(title=\"Training\", ema_alpha=0.2, max_points=100000)\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    os.makedirs(f\"saved_models/{opt.output_dir}\", exist_ok=True)\n",
    "    DEBUG_DIR = \"train_samples\"\n",
    "    os.makedirs(DEBUG_DIR, exist_ok=True)\n",
    "\n",
    "    # --------------------- 정규화 함수  ---------------------\n",
    "    def _to01(x: torch.Tensor) -> torch.Tensor:\n",
    "        # Heuristic: if outside [0,1] significantly, assume [-1,1] and map to [0,1]\n",
    "        if torch.min(x).item() < -0.05 or torch.max(x).item() > 1.05:\n",
    "            return ((x + 1.0) * 0.5).clamp(0.0, 1.0)\n",
    "        return x.clamp(0.0, 1.0)\n",
    "\n",
    "    #  --------------------- 평가 PSNR  ---------------------\n",
    "    @torch.no_grad()\n",
    "    def evaluate_psnr(forward, data_loader: DataLoader, max_batches: int = 16) -> float:\n",
    "        classifier.eval()\n",
    "        psnr_sum, count = 0.0, 0\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            if i >= max_batches:\n",
    "                break\n",
    "            input = batch['A_input'].to(device)\n",
    "            real = batch['A_target'].to(device)\n",
    "\n",
    "            fake = forward(input)\n",
    "            fake01 = _to01(fake)\n",
    "            real01 = _to01(real)\n",
    "            # MSE on [0,1]\n",
    "            mse = F.mse_loss(fake01, real01)\n",
    "            # Avoid log of zero\n",
    "            psnr = 10.0 * torch.log10(1.0 / (mse + 1e-12))\n",
    "            psnr_sum += psnr.item()\n",
    "            count += 1\n",
    "        return psnr_sum / max(1, count)\n",
    "\n",
    "    # --------------------- Training 관련 ------------------------\n",
    "    def gen_train(img_prev):\n",
    "        # Classifier를 통한 가중치 추출 + softmax\n",
    "        img = _to01(img_prev)\n",
    "        pred = classifier(img)\n",
    "        pred = F.softmax(pred, dim=1)\n",
    "\n",
    "        # LUT에 이미지를 넣고 결과들을 추출하여 stack\n",
    "        c0 = LUT0(img) # [B, 3, H, W]로 Tensor 구성\n",
    "        c1 = LUT1(img)\n",
    "        c2 = LUT2(img)\n",
    "        \n",
    "\n",
    "        cout = [c0, c1, c2]\n",
    "        LUT_all = torch.stack(cout, dim=1) # [B, L, 3, H, W] 여기서 L은 3\n",
    "\n",
    "        # B와 L을 Define 하기\n",
    "        B, C, H, W = img.shape\n",
    "        L = LUT_all.size(1) # LUT 개수\n",
    "\n",
    "        # 가중합 처리\n",
    "        out = (LUT_all * pred.view(B, L, 1, 1, 1)).sum(dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "    start_epoch = 0\n",
    "    if opt.epoch != 0:\n",
    "        LUTs = torch.load(f\"saved_models/{opt.output_dir}/LUTs_{opt.epoch}.pth\")\n",
    "        LUT0.load_state_dict(LUTs[\"0\"])\n",
    "        LUT1.load_state_dict(LUTs[\"1\"])\n",
    "        LUT2.load_state_dict(LUTs[\"2\"])\n",
    "        classifier.load_state_dict(torch.load(f\"saved_models/{opt.output_dir}/classifier_{opt.epoch}.pth\"))\n",
    "        opt_state = torch.load(f\"saved_models/{opt.output_dir}/optimizer_{opt.epoch}.pth\", map_location=device)\n",
    "        optimizer_G.load_state_dict(opt_state)\n",
    "        start_epoch = opt.epoch\n",
    "    else:\n",
    "        classifier.apply(weights_init_normal_classifier)\n",
    "        torch.nn.init.constant_(classifier.model[16].bias.data, 1.0)\n",
    "\n",
    "    prev_time = time.time()\n",
    "\n",
    "    for epoch in range(start_epoch, opt.n_epochs):\n",
    "        classifier.train()\n",
    "        loss_sum, psnr_sum = 0.0, 0.0\n",
    "\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            rgb  = batch['A_input'].to(device)   # [B,3,H,W]\n",
    "            real = batch['A_target'].to(device)  # [B,3,H,W]\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Forward\n",
    "            fake = gen_train(rgb)     # fake: [B,3,H,W]    \n",
    "            tv1, mn1 = TV3(LUT0)\n",
    "            tv2, mn2 = TV3(LUT1)\n",
    "            tv3, mn3 = TV3(LUT2) \n",
    "            \n",
    "            tv = tv1 + tv2 + tv3\n",
    "            mn = mn1 + mn2 + mn3\n",
    "            \n",
    "\n",
    "            ssim = SSIM().to(device)\n",
    "            ssim_loss = ssim(_to01(fake),_to01(real)).mean()\n",
    "            ciede2000_loss = delta_e00_from_srgb(_to01(fake),_to01(real)).mean()\n",
    "\n",
    "\n",
    "            mse_psnr = F.mse_loss(_to01(fake), _to01(real))\n",
    "            \n",
    "            total_loss = opt.lambda_mse*mse_psnr + opt.lambda_smooth*tv + opt.lambda_monotonicity*mn + ssim_loss*opt.lambda_ssim + opt.lambda_color*ciede2000_loss\n",
    "\n",
    "            # Backprop\n",
    "            total_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # PSNR (per-batch)\n",
    "            with torch.no_grad():\n",
    "                psnr = 10.0 * torch.log10(1.0 / (mse_psnr + 1e-12))\n",
    "                psnr_sum += psnr.item()\n",
    "                loss_sum += total_loss.item()\n",
    "\n",
    "            # ETA 표시\n",
    "            batches_done = epoch * len(train_loader) + i + 1\n",
    "            batches_left = opt.n_epochs * len(train_loader) - batches_done\n",
    "            iter_time = time.time() - prev_time\n",
    "            prev_time = time.time()\n",
    "            eta = datetime.timedelta(seconds=int(batches_left * max(iter_time, 1e-9)))\n",
    "\n",
    "\n",
    "            sys.stdout.write(\n",
    "                f\"\\r[Epoch {epoch}/{opt.n_epochs}] [Batch {i+1}/{len(train_loader)}] \"\n",
    "                f\"[psnr: {psnr_sum/(i+1):.6f}, loss: {total_loss.item():.6f}] \"\n",
    "                f\"[mse:{mse_psnr.item():.6f}] ETA: {eta}\"\n",
    "            )\n",
    "\n",
    "        # --- Validation (옵션: 간단히 train psnr 평균만 기록) ---\n",
    "        avg_psnr = psnr_sum / max(1, len(train_loader))\n",
    "        # Proper eval-mode PSNR using running stats (for BN) and without dropout\n",
    "        eval_psnr = evaluate_psnr(gen_train, train_loader, max_batches=8)\n",
    "        print(f\"\\n[Epoch {epoch}] train_psnr(mean): {avg_psnr:.6f} | eval_psnr: {eval_psnr:.6f} | total_loss:{total_loss:.6f}\")\n",
    "        plotter.update(epoch, avg_psnr, total_loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Save checkpoint\n",
    "        if epoch > 0:\n",
    "            LUTs = {str(i):lut.state_dict() for i,lut in enumerate([LUT0,LUT1,LUT2])}\n",
    "            torch.save(LUTs, f\"saved_models/{opt.output_dir}/LUTs_{epoch}.pth\")\n",
    "            torch.save(classifier.state_dict(), f\"saved_models/{opt.output_dir}/classifier_{epoch}.pth\")\n",
    "            torch.save(optimizer_G.state_dict(), f\"saved_models/{opt.output_dir}/optimizer_{epoch}.pth\")\n",
    "            with open(f\"saved_models/{opt.output_dir}/result.txt\",\"a\") as f:\n",
    "                f.write(f\"[PSNR:{avg_psnr:.6f}] [max PSNR:{eval_psnr:.6f}, epoch:{total_loss}]\\n\")\n",
    "\n",
    "            # 끝난 후 이미지로 저장하고 종료\n",
    "            plotter.finalize(\"training_metrics.png\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    freeze_support()\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d284efe0",
   "metadata": {},
   "source": [
    "## 7. Inference Code\n",
    "Training이 완료되었다면, 이제 추론을 통한 결과물을 확인 후 이를 평가하는 작업을 해야하는데, 아래는 그 작업을 실행하는 코드입니다.  [0,1] 정규화를 위한 _to01 함수와 output image 도출에 필요한 gen_inference 함수가 정의되어 있으며, 바로 밑에는 inference를 진행하는 loop 및 inference 결과를 저장하는 code가 작성되어 있습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41794534",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'saved_models/RGB_LUT_Classifier_Ckpt/LUTs_0.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 113\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmultiprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m freeze_support\n\u001b[0;32m    112\u001b[0m freeze_support()  \n\u001b[1;32m--> 113\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[31], line 40\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m     classifier \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m     38\u001b[0m     TV3 \u001b[38;5;241m=\u001b[39m TV3\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m---> 40\u001b[0m LUTs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaved_models/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/LUTs_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference_epoch\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m LUT0\u001b[38;5;241m.\u001b[39mload_state_dict(LUTs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     42\u001b[0m LUT1\u001b[38;5;241m.\u001b[39mload_state_dict(LUTs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\minami\\venv\\lib\\site-packages\\torch\\serialization.py:1479\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1477\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1479\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m   1480\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1481\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1482\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1483\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1484\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\minami\\venv\\lib\\site-packages\\torch\\serialization.py:759\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 759\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    760\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    761\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\minami\\venv\\lib\\site-packages\\torch\\serialization.py:740\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    739\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 740\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'saved_models/RGB_LUT_Classifier_Ckpt/LUTs_0.pth'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from models import *\n",
    "from datasets import ImageDataset_sRGB\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Parameters\n",
    "    opt = parser.parse_args()\n",
    "\n",
    "    def _to01(x: torch.Tensor) -> torch.Tensor:\n",
    "    # Heuristic: if outside [0,1] significantly, assume [-1,1] and map to [0,1]\n",
    "        if torch.min(x).item() < -0.05 or torch.max(x).item() > 1.05:\n",
    "            return ((x + 1.0) * 0.5).clamp(0.0, 1.0)\n",
    "        return x.clamp(0.0, 1.0)\n",
    "\n",
    "\n",
    "\n",
    "    # Cuda Device 지정 및 gradient 설정 금지 (inference이므로)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.set_grad_enabled(False)  # 전체 스크립트 no-grad\n",
    "\n",
    "    # ===== 1) LUT 및 Classifier 불러오기  =====\n",
    "    LUT0 = Generator3DLUT_identity(dim=33).to(device)\n",
    "    LUT1 = Generator3DLUT_zero().to(device)\n",
    "    LUT2 = Generator3DLUT_zero().to(device)\n",
    "    classifier = Classifier().to(device)\n",
    "    TV3 = TV_3D().to(device)\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        LUT0 = LUT0.cuda()\n",
    "        LUT1 = LUT1.cuda()\n",
    "        LUT2 = LUT2.cuda()\n",
    "        classifier = classifier.cuda()\n",
    "        TV3 = TV3.cuda()\n",
    "\n",
    "    LUTs = torch.load(f\"saved_models/{opt.model_dir}/LUTs_{opt.inf_epoch}.pth\")\n",
    "    LUT0.load_state_dict(LUTs[\"0\"])\n",
    "    LUT1.load_state_dict(LUTs[\"1\"])\n",
    "    LUT2.load_state_dict(LUTs[\"2\"])\n",
    "    classifier.load_state_dict(torch.load(f\"saved_models/{opt.model_dir}/classifier_{opt.inf_epoch}.pth\"))\n",
    "\n",
    "\n",
    "    # ===== 2) 데이터셋 =====\n",
    "    root = f\"data/{opt.dataset_name}\"\n",
    "    dataset = ImageDataset_sRGB(root, mode=\"test\")\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=opt.num_workers_eval,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "    )\n",
    "    \n",
    "    # ===== 3) 출력 폴더 =====\n",
    "    out_dir = os.path.join(\"images_new\", f\"{opt.model_dir}_{opt.inf_epoch}\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    # if opt.save_weights:\n",
    "    #     os.makedirs(os.path.join(out_dir, \"weights\"), exist_ok=True)\n",
    "\n",
    "\n",
    "    def gen_inference(img_prev):\n",
    "        # Classifier를 통한 가중치 추출 + softmax\n",
    "        classifier.eval()\n",
    "        img = _to01(img_prev)\n",
    "        pred = classifier(img)\n",
    "        pred = F.softmax(pred, dim=1)  # [B, L]\n",
    "\n",
    "        # LUT에 이미지를 넣고 결과들을 추출하여 stack\n",
    "        c0 = LUT0(img) # [B, 3, H, W]로 Tensor 구성\n",
    "        c1 = LUT1(img)\n",
    "        c2 = LUT2(img)\n",
    "        cout = [c0, c1, c2]\n",
    "        LUT_all = torch.stack(cout, dim=1) # [B, L, 3, H, W] 여기서 L은 3\n",
    "\n",
    "        # B와 L을 Define 하기\n",
    "        B, C, H, W = img.shape\n",
    "        L = LUT_all.size(1) # LUT 개수\n",
    "\n",
    "        # 가중합 처리\n",
    "        out = (LUT_all * pred.view(B, L, 1, 1, 1)).sum(dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "    # ===== 4) 추론 루프 =====\n",
    "    with torch.inference_mode():\n",
    "        loss_sum, psnr_sum = 0.0, 0.0\n",
    "        for batch in dataloader:\n",
    "            img = batch[\"A_input\"].to(device)# [B,3,H,W]\n",
    "            real = batch[\"A_target\"].to(device)\n",
    "            name = batch[\"input_name\"][0]\n",
    "            base = os.path.splitext(name)[0] + \".png\"\n",
    "\n",
    "            # (1) LUT 통과 후 이미지 생성\n",
    "            output = gen_inference(img)\n",
    "            \n",
    "            tv1, mn1 = TV3(LUT0)\n",
    "            tv2, mn2 = TV3(LUT1)\n",
    "            tv3, mn3 = TV3(LUT2) \n",
    "            \n",
    "            tv = tv1 + tv2 + tv3\n",
    "            mn = mn1 + mn2 + mn3\n",
    "               \n",
    "            mse_psnr = F.mse_loss(output, real)\n",
    "            total_loss = mse_psnr + opt.lambda_smooth*tv + opt.lambda_monotonicity*mn\n",
    "            \n",
    "            # PSNR (per-batch)\n",
    "            with torch.no_grad():\n",
    "                psnr = 10.0 * torch.log10(1.0 / (mse_psnr + 1e-12))\n",
    "                psnr_sum += psnr.item()\n",
    "                loss_sum += total_loss.item()\n",
    "            \n",
    "\n",
    "            # (2) 저장\n",
    "            save_image(output, os.path.join(out_dir, base), nrow=1, normalize=False)\n",
    "    \n",
    "            with open(f\"{out_dir}/result.txt\",\"a\") as f:\n",
    "                f.write(f\"[PSNR:{psnr:.6f}] [loss:{total_loss}]\\n\")\n",
    "\n",
    "    print(f\"[+] Inference done. Results saved in {out_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from multiprocessing import freeze_support\n",
    "    freeze_support()  \n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eebb7b",
   "metadata": {},
   "source": [
    "## 8. 수정점\n",
    "\n",
    "원본 코드에서 많은 점들을 수정했습니다. 그 양이 점점 방대해지고 있으므로 여기에 추가적으로 기록을 남겨 이후 DB 작성에 도움이 되고자 합니다.\n",
    "\n",
    "2025년 08월 19일까지 원본 코드에서 수정된 점 :\n",
    "\n",
    "#### RGBNIRtoRGB Class (추가 및 수정)\n",
    "    - 기존에는 RGB와 NIR 이미지를 받아, 그것을 concat한 이미지를 출력하는 단순한 Class으로써 추가되어, 4채널 입력을 3채널 R'G'B'로 바꾸어 출력하는 Class였음. (H의 조언)\n",
    "    - Brightness 등을 올리는 코드 등 여러 코드들을 이 Class 내부에서 테스트함.\n",
    "    - 그러나, 단순히 RGB 이미지와 NIR 이미지를 concat하니 사진의 채도, 명도와 같은 값들이 과도하게 바뀌어 합성된 사진이 단색으로 출력되는 문제가 발생.\n",
    "    - 따라서, NIR 이미지에서 선택적으로 전역 통계만을 torch.functional.adaptive_avg_pool2d 함수를 이용하여 추출, 이를 RGB의 Tensor 크기에 맞추고, 이후 RGB와 수정된 NIR를 conncat 및 softmax 정규화를 이용하니 합성된 사진이 비교적 정상적으로 출력되기 시작.\n",
    "    - 이후, Classifier와 3D LUT를 이용한 가중치 도출 기능 또한 통합, NIR 전역 평균을 이용한 YUV 영역에서 Y 값에 변화를 주어 output의 Brightness를 증가시키는 코드 또한 추가하여 현재에 이르게 됨.\n",
    "\n",
    "\n",
    "#### TrilinearInterpolation (수정)\n",
    "    - 기존의 Trilinear Interpolation Class의 경우 직접적으로 Pytorch 및 Cuda에 의존성이 매우 강한 함수라, 해당 프로그램을 구동하기 위해서는 이 Class의 수정이 필수적이었음.\n",
    "    - 따라서, 현재 Pytorch 2.7.x 및 Cuda 12.1 이상 버전과 호환되게 @staticmethod 추가, .apply로 직접 Class 호출, 변수의 contiguous 보장 등과 같은 코드 문법들을 1차적으로 수정함.\n",
    "    - 또한, Trilinear Interpolation 기능을 정의하고 사용하기 위한 C++/C code들인 Trilinear.cpp 내부 6개 파일의 코드들도 수정했는데, 수정 사항은 아래와 같음.\n",
    "        - Pytorch 2.7.x 및 Cuda 12.1 이상 버전과 호환성을 가지게 수정, 특히, 코드들 내부에서 at::cuda::getCurrentCUDAStream() 항목을 호출하기 위한 binary가 달라졌기에, 바뀐 binary를 불러와서 이 문제를 해결함.\n",
    "        - 단순히 R,G,B 3채널만을 이용하여 계산을 실행하는 기존의 코드를 수정하여, Batch를 인식하는 기능을 추가함. 따라서 [B,H,W]와 같은 형식의 Value가 들어감.\n",
    "    - 중간에 시행착오가 많았으나, 기존 코드를 최대한 이용할 수 있게 수정하는 방향으로 진행함.\n",
    "\n",
    "#### ~~Generator_3DLUT (Rollback)~~\n",
    "\n",
    " \n",
    "#### ImageDataset_sRGB (수정)\n",
    "    - NIR 이미지 input 추가, 8bit NIR image file을 float16으로 바꾸는 코드 추가(정규화), __getitem__ 에 filp 및 Crop, input image의 brightness를 무작위로 증가 혹은 감소시켜 학습 능률을 올리는 기능 추가.\n",
    "    - test 모드일 경우, Target image를 참고하지 않고 Evaluation을 진행하며, train 모드일 경우 Target image를 참고하여 PSNR을 도출해내는 조건문 추가.\n",
    " \n",
    "#### ~~Optimizer (rollback)~~\n",
    " \n",
    "#### Training Code(수정)\n",
    "    - generator_train, generator_eval, calculate_PSNR 항목 제거. 이제 이 기능들은 RGBNIRtoRGB Class에서 Generator와 Classifier의 역할을 동시에 처리.\n",
    "        - 따라서, LUT를 load하는 방식 및 Checkpoint 파일을 저장하는 방식 등이 세세하게 달라짐. 기존에는 LUT, Classifier, Optimizer 등의 파일들이 각각 따로 저장되고 load 되었다면, 이젠 Generator 파일 하나로 통합하여 한 번에 Load할 수 있게 함.\n",
    "    - import하는 function들이 Cuda를 지원하는 Device를 사용할 수 있도록 .to(device) 항목 추가\n",
    "    - NIR image를 입력하는 필드 추가.\n",
    "    - 기존에 MAX_PSNR 등을 계산하는 항목 등을 제거하는 대신, 각 Loss 값을 확인할 수 있는 항목을 추가함.\n",
    "    - Loss Function을 정의하는 부분에서 하드웨어적으로 구현이 어려운 Monotonic Loss와 Smoothing Loss 항목을 제거하고, MSELoss, L1Loss, Chroma + (Entropy) 항목들로 이루어진 새로운 Loss Function을 정의함. Entropy는 Model의 LUT 사용을 균형잡히게 만들어 학습이 고르게 이루어지도록 도움. -> 추가적인 수정이 있을 수 있음.\n",
    "\n",
    " \n",
    "#### Inference Code (수정)\n",
    "    - Post processing 관련 function 2종을 추가하여, 출력의 화이트밸런스, Sharpness 등의 미세 조절을 선택할 수 있게 만들었음.\n",
    "    - NIR image를 입력하는 필드 추가.\n",
    "\n",
    "\n",
    "\n",
    "2025년 09월  03일 수정된 부분\n",
    "\n",
    "Quadlinear Interpolation → Trilinear Interpolation으로 다시 원복 (dim 4 → 3)\n",
    "\n",
    "- 기존의 C code들도 이에 맞게 재수정 필요 -> 백업본으로 원복.\n",
    "\n",
    "Generator4DLUT 계열 → 3DLUT로 원복 (dim 4 → 3)\n",
    "\n",
    "RGBNIRtoRGB 주석 처리\n",
    "\n",
    "Dataset, Training, Inference 항목에서 NIR 입력을 더 이상 받지 않음.\n",
    "\n",
    "기존처럼 LUT + Classifier 구성으로 갈 지, 아니면 지금처럼 Generator 구성으로 갈 지 고민이 필요 -> 더 이상 Generator가 Classifier와 LUT가 합쳐진 구성이 아님.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
